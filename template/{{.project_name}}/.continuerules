# Databricks Asset Bundles (DAB) Rules

## 1. Bundle Structure and Configuration

- Every bundle **must** have a `databricks.yml` at its root. Additional configuration files can be included via the `include` key.
- Use modular configuration: separate resources, targets, and artifacts into their own files when possible for maintainability.
- Use the `resources` key to define Databricks resources (jobs, pipelines, clusters, etc.) using the corresponding REST API payload structure in YAML.
- Use the `targets` key to define deployment environments (e.g., dev, staging, prod). Each target can override resources, artifacts, variables, and workspace settings.
- Only one target can have `default: true`.
- Use the `variables` key to define reusable, parameterized values. Reference variables with `${var.<name>}`.
- Use substitutions for dynamic values (e.g., `${bundle.name}`, `${workspace.current_user.userName}`).

## 2. Resource Definition and Management

- Define jobs, pipelines, clusters, and other resources under the `resources` key.
- For jobs and pipelines, use the same YAML structure as the Databricks REST API (see @POST /api/2.1/jobs/create).
- Use `job_clusters` for reusable cluster definitions within jobs.
- Use `tasks` to define job steps. Each task must have a unique `task_key`.
- For pipelines, define clusters under `clusters` and libraries under `libraries`.
- Use `depends_on` in tasks to specify execution order.
- Use `job_cluster_key` or `existing_cluster_id` to assign clusters to tasks.

## 3. Artifact Management

- Use the `artifacts` key to define buildable artifacts (e.g., Python wheels, JARs).
- For Python wheels, set `type: whl` and specify `build` and `path` as needed.
- Artifacts can be overridden per target under `targets.<target>.artifacts`.
- Use the `libraries` key in tasks to install wheel, JAR, PyPI, Maven, or requirements.txt dependencies.
- For private artifacts, download them locally and reference them in the bundle. Optionally, upload to Unity Catalog volumes for secure sharing.

## 4. Workspace and Deployment Paths

- Use the `workspace` key to configure deployment paths (`host`, `root_path`, `artifact_path`, `file_path`).
- Prefer using `host` over `profile` for portability.
- For production, set `root_path` to a shared, non-user-specific location (e.g., `/Shared/.bundle/prod/${bundle.name}`).
- Use substitutions to construct dynamic paths.

## 5. Authentication and Security

- For development, use OAuth U2M authentication and configuration profiles.
- For CI/CD and production, use service principals (OAuth M2M) and environment variables for secrets.
- Never hard-code secrets or tokens in bundle files.
- Use the `run_as` key to specify the identity for workflow execution. For production, prefer service principals.
- Set permissions at the top level or per resource using the `permissions` key. Avoid overlapping permissions between top-level and resource-level definitions.

## 6. CI/CD and Collaboration

- Store bundles in version control (e.g., Git). Use Git folders in Databricks for collaboration.
- Use the `git` key to annotate bundle deployments with repository and branch info.
- For air-gapped environments, use the Databricks CLI Docker image and mount the bundle directory.
- Use the `bundle generate` and `bundle deployment bind` commands to migrate and synchronize existing workspace resources with bundle configuration.

## 7. Development and Deployment Modes

- Use `mode: development` for dev targets. This enables:
  - Resource name prefixing (e.g., `[dev username]`)
  - Paused schedules/triggers
  - Concurrent job runs
  - Disabled deployment lock for faster iteration
- Use `mode: production` for prod targets. This enforces:
  - Validation of DLT pipeline development flags
  - Git branch checks (if specified)
  - Service principal enforcement (recommended)
  - No cluster override via CLI
- Use `presets` to customize deployment behaviors (e.g., `name_prefix`, `tags`, `jobs_max_concurrent_runs`, `trigger_pause_status`).

## 8. Python Support for Bundles

- You can define jobs and pipelines in Python using the `databricks-bundles` package.
- Enable Python support in `databricks.yml` under `experimental.python`.
- Use a virtual environment (`venv_path`) and specify resource loader functions.
- Use mutators to programmatically modify jobs during deployment (e.g., add email notifications).
- Use the "View as code" feature in the UI to convert existing jobs to Python or YAML for inclusion in bundles.

## 9. Error Handling, Validation, and Best Practices

- Always run `databricks bundle validate` after changes to ensure configuration correctness.
- Use the `bundle schema` command to generate JSON schema for IDE validation.
- Use `databricks bundle deploy` to push changes to the workspace, and `databricks bundle run <job/pipeline>` to execute resources.
- Use `databricks bundle destroy` to clean up deployed resources.
- Always ask for a profile to add to the commands, `databricks bundle deploy --profile <insert profile name>`.
- For resource overrides (e.g., clusters, tasks), settings in the target take precedence over top-level definitions.
- For artifacts, settings in the target override top-level artifact settings.
- For clusters and tasks, use matching keys (`job_cluster_key`, `task_key`, `label`) to join and override settings between top-level and target-level definitions.

## 10. Security and Compliance

- Do not store sensitive data (e.g., secrets, tokens) in bundle files.
- Use secret scopes and Unity Catalog for secure data and artifact storage.
- For production, enforce `run_as` with service principals and set explicit permissions.
- Avoid using DBFS root for library storage in Databricks Runtime 15.1+; use workspace files or Unity Catalog volumes instead.

## 11. Collaboration in the Workspace

- Enable workspace files and serverless compute for full bundle functionality.
- Use Git folders for collaborative bundle development.
- Share bundles by sharing the parent Git folder.
- Use the Databricks UI or CLI for rapid iteration, validation, and deployment.

## 12. Custom Templates and Extensibility

- Use `databricks bundle init <template>` to create bundles from default or custom templates.
- Custom templates must include a `databricks_template_schema.json` and configuration templates using Go templating syntax.
- Use template helpers (e.g., `{{user_name}}`, `{{workspace_host}}`) for dynamic configuration.

## 13. Common Patterns and Examples

- Use `notebook_task`, `spark_python_task`, `python_wheel_task`, `sql_task`, `pipeline_task`, `dbt_task`, `dashboard_task`, `condition_task`, `for_each_task`, and `run_job_task` as needed in job definitions.
- Reference artifacts and libraries using relative paths or workspace/volume paths.
- Use `depends_on` for task dependencies and `parameters` for job parameterization.
- For DLT pipelines, set `serverless: true` and configure clusters and libraries as needed.

---

**References:**
- @Databricks Asset Bundles Documentation
- @Bundle configuration reference
- @Bundle resource examples
- @Bundle templates
- @Bundle library dependencies
- @Bundle authentication
- @Bundle run identity
- @Bundle permissions
- @Bundle variables and substitutions

# Databricks DLT (Delta Live Tables) Pipeline Rules

This rule provides comprehensive guidance for building robust, maintainable, and production-ready DLT pipelines in Databricks, covering both Python and SQL interfaces. It addresses pipeline structure, data ingestion, transformation, data quality, CDC, deployment, monitoring, and operational best practices.

---

## 1. Pipeline Structure and Source Code Organization

- **Use Unity Catalog** for all new pipelines. Prefer serverless pipelines unless specific compute customization is required.
- **Organize pipeline code** using the multi-file editor or workspace files. Place source code in a dedicated root folder; avoid mixing exploratory notebooks with production pipeline code.
- **Separate transformation logic** into modular files (e.g., `data_sources/`, `transformations/`, `utilities/`).
- **Do not mix Python and SQL in the same file**. Each file should contain only one language.
- **Parameterize environment-specific values** (e.g., data paths, schema names) using pipeline configuration parameters, accessed via `spark.conf.get()` in Python or `${param}` in SQL.

---

## 2. Dataset Types and When to Use Them

- **Streaming Tables**: Use for ingesting and incrementally processing append-only or streaming data sources (e.g., Kafka, cloud files). Suitable for low-latency, high-throughput workloads.
- **Materialized Views**: Use for batch or incremental transformations, aggregations, or when results need to be cached and shared. Materialized views are always correct and can be incrementally refreshed if source tables support it.
- **Views**: Use for intermediate, non-persistent transformations or validation steps. Views are recomputed on each query and are not published outside the pipeline.
- **Temporary Tables**: Use for intermediate results not intended for external consumption. Declare with the `TEMPORARY` keyword.

---

## 3. Data Ingestion Patterns

- **Use Auto Loader** (`cloudFiles` in Python, `read_files` in SQL) for scalable, incremental ingestion from cloud object storage.
- **For message buses** (Kafka, Kinesis, Event Hubs), use the appropriate Spark Structured Streaming connectors. Store sensitive credentials in Databricks secrets and retrieve them securely in code.
- **For CDC (Change Data Capture)**, use the DLT `APPLY CHANGES` or `APPLY CHANGES FROM SNAPSHOT` APIs for robust, out-of-order event handling and SCD Type 1/2 support.
- **Always define schemas explicitly** for streaming sources when possible. Use schema inference and evolution (`from_json` with `schemaLocationKey`) only when necessary, and provide schema hints to control data types.

---

## 4. Data Quality and Expectations

- **Define expectations** on all critical datasets using DLT expectations (`@dlt.expect`, `@dlt.expect_or_drop`, `@dlt.expect_or_fail` in Python; `EXPECT` constraints in SQL).
- **Group reusable expectations** in Python using `expect_all`, `expect_all_or_drop`, or `expect_all_or_fail` with a dictionary of rules.
- **Store expectation definitions separately** (e.g., in a Delta table or Python module) for portability and maintainability.
- **Use tags** to group and filter expectations for different data quality domains.
- **Monitor data quality metrics** via the DLT UI or by querying the event log.
- **Quarantine invalid records** using temporary tables and views when records should be tracked but not dropped or failed.

---

## 5. Change Data Capture (CDC) and SCD Patterns

- **Use `APPLY CHANGES`** for ingesting CDC feeds (e.g., Debezium, Delta CDF) into streaming tables.
    - Specify `keys`, `sequence_by`, `apply_as_deletes`, and `except_column_list`.
    - Use `stored_as_scd_type=1` for SCD Type 1 (overwrite), `stored_as_scd_type=2` for SCD Type 2 (history).
    - For SCD Type 2, use `track_history_except_column_list` or `track_history_column_list` to control which columns are tracked for history.
- **Do not use `MERGE INTO` with `foreachBatch`** for CDC unless absolutely necessary; it is error-prone and does not handle out-of-order or duplicate events robustly.
- **For snapshot-based CDC**, use `APPLY CHANGES FROM SNAPSHOT` with periodic or historical snapshots.

---

## 6. Transformation and Aggregation Patterns

- **Use streaming tables for incremental aggregations** (e.g., count, sum, min, max) when the number of groups is limited.
- **Use materialized views for complex aggregations** or when transformations require full recomputation or non-append-only sources.
- **Combine streaming tables and materialized views** in a single pipeline for medallion architectures (bronze/silver/gold).
- **For stateful operations** (windowed aggregations, stream-stream joins, deduplication), always define watermarks to bound state and avoid OOM errors.
    - Use `.withWatermark("timestamp", "interval")` in Python or `WATERMARK` clause in SQL.
- **For stream-static joins**, be aware that static tables are snapshotted at the start of the stream and do not reflect later changes unless a full refresh is triggered.

---

## 7. Pipeline Configuration and Compute

- **Use serverless pipelines** with Unity Catalog for most workloads. Serverless manages compute, autoscaling, and vertical scaling automatically.
- **For classic compute pipelines**, enable enhanced autoscaling and set appropriate min/max workers.
- **Set the default catalog and schema** in pipeline configuration. Use fully qualified table names for cross-catalog/schema references.
- **Parameterize pipeline settings** (e.g., data paths, trigger intervals) using the `configuration` object in the pipeline JSON or UI.
- **For production pipelines**, use production mode for better error handling and retries; use development mode for interactive development and testing.
- **Do not attempt to set cluster attributes that are managed by DLT** (e.g., Spark version, cluster name, auto-termination).

---

## 8. Dependency Management

- **Install Python dependencies** using `%pip install` in a separate cell at the top of each pipeline notebook. Do not mix `%pip install` with other code in the same cell.
- **All notebooks in a pipeline share the same library environment**; do not attempt to use different library versions in the same pipeline.
- **Do not use JVM libraries or Scala/Java code** in DLT pipelines; only Python and SQL are supported.
- **Minimize use of init scripts**; if required, automate testing to detect runtime upgrade issues.

---

## 9. Security and Access Control

- **Use Unity Catalog for fine-grained access control**. Grant `SELECT` and `REFRESH` privileges on streaming tables and materialized views as needed.
- **Pipeline updates run as the pipeline owner**. Use a service principal as the owner for production pipelines.
- **Store sensitive credentials** (e.g., cloud storage keys, Event Hubs keys) in Databricks secrets and retrieve them securely in code.
- **By default, only the pipeline owner and admins can view driver logs**. To allow others, set `spark.databricks.acl.needAdminPermissionToViewLogs=false` in pipeline configuration.

---

## 10. Operational Best Practices

- **Monitor pipelines** using the DLT UI, event log, and query history. Use event hooks for custom monitoring and alerting.
- **Configure email notifications** for pipeline success/failure events.
- **Use the DLT event log** to audit lineage, data quality, autoscaling events, and errors.
- **Use selective refresh** to update only specific tables during development or after failures.
- **Avoid unnecessary full refreshes**; use incremental refresh whenever possible. Set `pipelines.reset.allowed=false` on tables that should not be fully refreshed.
- **For GDPR or compliance deletes**, use DML statements and `REORG ... APPLY (PURGE)` followed by `VACUUM` to physically remove data from streaming tables/materialized views with deletion vectors enabled.

---

## 11. Development and Testing

- **Develop pipeline code in notebooks, workspace files, or local IDEs**. Use the Databricks DLT Python stub for local development (syntax checking, autocomplete).
- **Test pipelines with sample or anonymized data**. Create separate pipelines for dev/test/prod, parameterizing data sources as needed.
- **Validate pipeline code** using the "Validate" update before running full updates.
- **Use Databricks Asset Bundles** for source-controlled, portable pipeline configurations and automated deployment.
- **Use the multi-file editor** for large or complex pipelines to organize code and facilitate collaboration.

---

## 12. Limitations and Known Issues

- **DLT pipelines are limited to 100 concurrent updates per workspace**.
- **Each DLT dataset can be defined only once**; only streaming tables with append flows can have multiple flows writing to them.
- **Identity columns are not supported with `APPLY CHANGES` targets** and may be recomputed in materialized views.
- **Materialized views and streaming tables created by DLT cannot be shared via Delta Sharing**.
- **Time travel is only supported on streaming tables, not materialized views**.
- **The `pivot()` function is not supported in DLT**.
- **JVM libraries and custom Spark versions are not supported**.
- **Materialized views and streaming tables are only accessible to Databricks clients/applications unless written to external Delta tables via DLT sinks**.

---

## 13. Example Patterns

### Streaming Table Ingestion (Python)

```python
import dlt

@dlt.table
def customers_bronze():
    return (
        spark.readStream.format("cloudFiles")
        .option("cloudFiles.format", "json")
        .load("/Volumes/path/to/files")
    )
```

### Materialized View with Expectations (SQL)

```sql
CREATE OR REFRESH MATERIALIZED VIEW customers_prepared
(CONSTRAINT valid_id EXPECT (id IS NOT NULL) ON VIOLATION DROP ROW)
AS SELECT * FROM customers_bronze;
```

### CDC with APPLY CHANGES (Python)

```python
import dlt
from pyspark.sql.functions import col, expr

@dlt.view
def users():
    return spark.readStream.table("cdc_data.users")

dlt.create_streaming_table("target")
dlt.apply_changes(
    target="target",
    source="users",
    keys=["userId"],
    sequence_by=col("sequenceNum"),
    apply_as_deletes=expr("operation = 'DELETE'"),
    except_column_list=["operation", "sequenceNum"],
    stored_as_scd_type=2
)
```

### Data Quality with Reusable Expectations (Python)

```python
rules = {
    "valid_id": "id IS NOT NULL",
    "valid_email": "email LIKE '%@%'"
}

@dlt.table
@dlt.expect_all_or_drop(rules)
def clean_customers():
    return spark.read.table("raw_customers")
```

---

## 14. References

- @DLT Concepts
- @DLT Python Reference
- @DLT SQL Reference
- @DLT CDC
- @DLT Expectations
- @DLT Properties
- @DLT Event Log
- @DLT Asset Bundles

---

**Follow these rules to ensure DLT pipelines are robust, maintainable, secure, and aligned with Databricks best practices.**